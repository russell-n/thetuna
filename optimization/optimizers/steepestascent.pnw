The Steepest Ascent Hill Climber (With Replacement)
===================================================

*Steepest Ascent Hill-Climbing With Replacement* makes the search more aggresive than a regular hill-climber by sampling multiple times around the current candidate solution [EOM]_.

Contents:

   * :ref:`SteepestAscent Class <optimization-optimizers-steepestascent>`

For examples:
       
   * :ref:`Normal Distribution Example <optimization-optimizers-steepestascent-normalexample>`
   * :ref:`Needle in a Haystack Example <optimization-optimizers-steepestascent-needleinahaystack>`   
   * :ref:`Gaussian Convolution Noisy Example <optimization-optimizers-steepestascent-gaussianconvolution>`
   * :ref:`Gaussian Convolution Normal Example <optimization-optimizers-steepestascent-gaussianconvolution-normal>`

<<name='imports', echo=False>>=
# this package
from optimization.optimizers.baseclimber import BaseClimber
@

.. _optimization-optimizers-steepestascent:

SteepestAscent Class
--------------------

.. uml::

   BaseClimber <|-- SteepestAscent

.. currentmodule:: optimization.optimizers.steepestascent
.. autosummary::
   :toctree: api

   SteepestAscent
   SteepestAscent.__call__

<<name='SteepestAscent', echo=False>>=
class SteepestAscent(BaseClimber):
    """
    Steepest Ascent with Replacement
    """
    def __init__(self, local_searches, emit=False, solutions_storage=None,
                 *args, **kwargs):
        """
        Steepest Ascent Constructor

        :param:

         - `local_searches`: number of tweaks per repetition
         - `emit`: if True, print candidates as they appear
         - `solutions_storage`: object with `append` method to store solutions
        """
        super(SteepestAscent, self).__init__(*args, **kwargs)
        self.emit = emit
        self.local_searches = local_searches
        self._solutions = solutions_storage
        return

    @property
    def solutions(self):
        """
        Object to store the solutions (defaults to a list)
        """
        if self._solutions is None:
            self._solutions = []
        return self._solutions

    def __call__(self):
        """
        Runs the algorithm (sets self.solutions as side-effect)

        :return: best solution found
        """
        current = self.solution
        
        # this sets the output value for the first check
        self.quality(current)
        
        while not self.stop_condition(self.solution):
            candidate = self.tweak(current)
            
            for search in xrange(self.local_searches):
                # search around the current spot
                new_candidate = self.tweak(current)
                if self.quality(new_candidate) > self.quality(candidate):
                    candidate = new_candidate
            current = candidate
            if self.quality(current) > self.quality(self.solution):
                self.solutions.append(current)
                if self.emit:
                    print current
                self.solution = current
        return self.solution
# end SteepestAscent    
@

This optimizer only has one real parameter to tune (``local_searches``) which decides how much it looks around each candidate. If this is small it will act more like a regular hill-climber (so the data has to have more information than noise) but if it is large it will be less likely to go off in the wrong direction. The Tweak used is what's responsible for most of the exploration this does. With GaussianConvolution, changing :math:`\sigma^2` to something larger will cause it to jump more often. If both the number of local searches and the spread are large, you end up with `evolutionary pressure <http://en.wikipedia.org/wiki/Evolutionary_pressure>`_ where there will be high mutation but the aggressive local searching will tend to weed out the bad variants.
